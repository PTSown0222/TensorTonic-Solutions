import numpy as np

def softmax(x, axis=-1):
    """Provided: Softmax function."""
    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e_x / np.sum(e_x, axis=axis, keepdims=True)

def layer_norm(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    """
    Apply layer normalization.
    """
    muy = np.mean(x, axis=-1, keepdims=True)
    var = np.var(x, axis=-1, keepdims=True)
    norm = gamma * (x - muy) / np.sqrt(var + eps) + beta
    
    return norm

def multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                         W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray,
                         W_o: np.ndarray, num_heads: int) -> np.ndarray:
    """
    Multi-head attention.
    """
    batch, seq_len, d_model = Q.shape
    d_k = d_model // num_heads

    Q_proj = Q @ W_q  
    K_proj = K @ W_k
    V_proj = V @ W_v

    Q_split = Q_proj.reshape(batch, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)
    K_split = K_proj.reshape(batch, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)
    V_split = V_proj.reshape(batch, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)

    attn_score = (Q_split @ K_split.swapaxes(-1, -2)) / np.sqrt(d_k)
    attn_prob = softmax(attn_score, axis=-1)
    context = attn_prob @ V_split

    context_concat = context.transpose(0, 2, 1, 3).reshape(batch, seq_len, d_model)
    output = context_concat @ W_o 
    
    return output

def feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray,
                 W2: np.ndarray, b2: np.ndarray) -> np.ndarray:
    """
    Position-wise feed-forward network.
    """
    hidden = x @ W1 + b1
    relu_out = np.maximum(0, hidden)
    output = relu_out @ W2 + b2
    return output

def encoder_block(x: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray,
                  W_o: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray,
                  b2: np.ndarray, gamma1: np.ndarray, beta1: np.ndarray,
                  gamma2: np.ndarray, beta2: np.ndarray, num_heads: int) -> np.ndarray:
    """
    Complete encoder block: MHA + FFN with residuals and layer norms.
    """
    # Multi-Head Self-Attention
    attn_out = multi_head_attention(Q=x, K=x, V=x, 
                                    W_q=W_q, W_k=W_k, W_v=W_v, W_o=W_o, 
                                    num_heads=num_heads)
    # Add & Norm 1
    out_1 = layer_norm(x + attn_out, gamma1, beta1)

    # Feed Forward Network (FFN)
    ffn_out = feed_forward(out_1, W1, b1, W2, b2)

    # add and norm 2
    out_final = layer_norm(out_1 + ffn_out, gamma2, beta2)

    return out_final